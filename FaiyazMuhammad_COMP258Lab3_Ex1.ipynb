{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1: Using Data API - Loading and Preprocessing Data with TensorFlow"
      ],
      "metadata": {
        "id": "mOXiQM6FX6ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ucimlrepo\n",
        "import tensorflow as tf\n",
        "from tensorflow import feature_column\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from ucimlrepo import fetch_ucirepo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10ocZveNYjII",
        "outputId": "adec619f-056d-44f6-c845-ae0cdac67014"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.3-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adult_dataset = fetch_ucirepo(id=2)\n",
        "X = adult_dataset.data.features.copy()\n",
        "y = adult_dataset.data.targets"
      ],
      "metadata": {
        "id": "zllECVeVYlok"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "missing_values = X.isnull().sum()\n",
        "print(\"Missing values in each column:\\n\", missing_values)\n",
        "\n",
        "# Handling Missing Values\n",
        "X.fillna(method='ffill', inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D2BqGoYe91J",
        "outputId": "66fcda8e-c20c-4135-ddbb-ba802ae19fa1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in each column:\n",
            " age                 0\n",
            "workclass         963\n",
            "fnlwgt              0\n",
            "education           0\n",
            "education-num       0\n",
            "marital-status      0\n",
            "occupation        966\n",
            "relationship        0\n",
            "race                0\n",
            "sex                 0\n",
            "capital-gain        0\n",
            "capital-loss        0\n",
            "hours-per-week      0\n",
            "native-country    274\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
        "for col in categorical_columns:\n",
        "    X[col] = pd.Categorical(X[col]).codes\n"
      ],
      "metadata": {
        "id": "5Jso_IoljwMs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations and Standardization\n",
        "# Here we are assuming all numerical columns need standardization\n",
        "numerical_columns = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
        "scaler = StandardScaler()\n",
        "X[numerical_columns] = scaler.fit_transform(X[numerical_columns])\n"
      ],
      "metadata": {
        "id": "WGOpWpa6jx_T"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Splitting Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "xwUaBIDoj155"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Creating TensorFlow Dataset\n",
        "def df_to_dataset(dataframe, labels, shuffle=True, batch_size=32):\n",
        "    dataframe = dataframe.copy()\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "    ds = ds.batch(batch_size)\n",
        "    return ds\n",
        "\n",
        "train_ds = df_to_dataset(X_train, y_train)\n",
        "test_ds = df_to_dataset(X_test, y_test, shuffle=False)"
      ],
      "metadata": {
        "id": "175vDx1Lj6cZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ArEAvO4TkC_A"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}